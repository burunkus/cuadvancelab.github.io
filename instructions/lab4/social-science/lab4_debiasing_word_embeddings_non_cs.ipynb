{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Debiasing word embeddings\n",
        "\n",
        "Word embedding are word vectors that have meaning, word vectors similar to each other will be close to each other in a vector space. \n",
        "\n",
        "**After completing this lab you will be to:**\n",
        "\n",
        "- Measure similarity of word vectors using cosine similarity\n",
        "- Solve word analogy probelms such as Man is to Woman as Boy is to ____ using word embeddings\n",
        "- Reduce gender bias in word embeddings by modifying word embeddings to remove gender stereotypes, such as the association between the words *receptionist* and *female*"
      ],
      "metadata": {
        "id": "A7lACZZxKsY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='darkblue'>Word embeddings</font>\n",
        "\n",
        "Word embedding is a method used to represent words as vectors. They are populary used in machine learning and natural language processing tasks. Despite their success in downstream tasks such as cyberbullying, sentiment analysis, and question retrieval, they exhibit gender sterotypes which raises concerns because their widespread use can amplify these biases. \n",
        "\n",
        "\n",
        "Word embeddings are trained using a machine learning algorithm on word co-occurance using a text dataset. After training , each word will be represented as a vector. **You can think of a vector as a list of numbers. When convenient we will refer to a vector of a word using an arrow on top of the word. But in most part of the lab the arrow will be omitted for simplicity.**\n",
        "\n",
        "#### Word embedding properties:\n",
        "* Words with similar semantic meaning will be close to each other \n",
        "* The difference between word embedding vectors can represent relationships between words. For example, given the analogy \"man is to King as woman is to $x$\" (denoted as $man:king :: woman:x$), by doing simple arithmetic on the embedding vectors, we find that $x = queen$ is the best answer because $\\vec{man} - \\vec{woman} ≈ \\vec{king} - \\vec{queen}$. For the analogy $Paris:France :: Nairobi:x$, finds that $x = Kenya$. These embeddings can also amplify sexism implicit in text. For instance, $\\vec{man} - \\vec{woman} ≈ \\vec{computer \\space programmer} - \\vec{homemaker}$. The same system that produced reasonable answers to the previous examples offensively answers \"man is to computer programmer as woman is to $x$\" with $x = homemaker$.\n",
        "\n",
        "Run the following cell to load the required modules. "
      ],
      "metadata": {
        "id": "UTlroLtZPGwh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIgOC7jfKe6w",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='darkblue'>Download and Load word vectors</font>\n",
        "Due to the computational resources required to train word embeddings, we will be using a pre-trained word embeddings known as GloVe. \n",
        "\n",
        "Run the following cells to download and load the word embeddings."
      ],
      "metadata": {
        "id": "5jgtMxNajovM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def download_glove_vectors():\n",
        "    '''\n",
        "    Download the GloVe vectors\n",
        "    Arguments:\n",
        "        None\n",
        "    Returns:\n",
        "        file_name (String): The absolute path of the downloaded 50-dimensional\n",
        "        GloVe word vector representations\n",
        "    '''\n",
        "    \n",
        "    if not Path('data').is_dir():\n",
        "        print(\"Downloading the word embeddings ...\")\n",
        "        !wget --quiet https://nlp.stanford.edu/data/glove.6B.zip\n",
        "        print(\"Word embeddings downloaded.\")\n",
        "    \n",
        "        # Unzip it\n",
        "        print(\"Unzipping the downloaded file ...\")\n",
        "        !unzip -q glove.6B.zip -d data/ \n",
        "        print(\"File unzipped.\")\n",
        "\n",
        "    return '/content/data/glove.6B.50d.txt'"
      ],
      "metadata": {
        "id": "rP6R3VgXmMwQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def get_glove_vectors(glove_file):\n",
        "    '''\n",
        "    Read the word vectors in glove_file\n",
        "    Arguments:\n",
        "        glove_file (String): The absolute path to the downloaded glove word embeddings\n",
        "    Returns:\n",
        "        words (Set): The words (vocabulary) in the pretrained glove word embeddings\n",
        "        word_to_vector_map (Dict): A dictionary mapping the each word to its embedding vector \n",
        "    '''\n",
        "\n",
        "    words = set()\n",
        "    word_to_vector_map = {}\n",
        "    with open(glove_file, 'r') as file_handle:\n",
        "        for line in file_handle:\n",
        "            line = line.strip().split()\n",
        "            current_word = line[0]\n",
        "            words.add(current_word)\n",
        "            current_word_vector = line[1:]\n",
        "            word_to_vector_map[current_word] = np.array(current_word_vector, dtype=np.float64)\n",
        "\n",
        "    return words, word_to_vector_map"
      ],
      "metadata": {
        "id": "NdjBx4rsjgTp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and load words as vectors(a list of numbers)"
      ],
      "metadata": {
        "id": "Ht13FbieMkm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Load sets of words in the vocabulary and a dictionary mapping words to their GloVe vectors \n",
        "words, word_to_vector_map = get_glove_vectors(download_glove_vectors())"
      ],
      "metadata": {
        "id": "y0iu6d7On-Ei",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many words are in our vocabulary"
      ],
      "metadata": {
        "id": "emgI-2iqQ2mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "print(f\"Vocabulary contains {len(word_to_vector_map)} words\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "OTVWuJhTQ_e3",
        "outputId": "55ca4cd1-7cbd-4fa3-f908-b1da0386fab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary contains 400000 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "View a few words by running the cell below"
      ],
      "metadata": {
        "id": "K1ykEle0M0XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "print(f'Doctor is represented as: \\n {word_to_vector_map[\"doctor\"]}')\n",
        "print()\n",
        "print(f'School is represented as: \\n {word_to_vector_map[\"school\"]}')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aCGnVRgcMzY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try your own output. Run the cell below and enter a word then **hit enter or return** on your keyword."
      ],
      "metadata": {
        "id": "v6yVbc7UP3gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "text = widgets.Text(description='Enter a word:', value='', disabled=False)\n",
        "display(text)\n",
        "\n",
        "def display_input_text_vector(input):\n",
        "    try:\n",
        "        vector = word_to_vector_map[input.value]\n",
        "    except KeyError:\n",
        "        print(f'{input.value} is not in our vocabulary. Give another word a shot.')\n",
        "        return\n",
        "    print(f'{input.value} is represented as: \\n{vector}')\n",
        "\n",
        "text.on_submit(display_input_text_vector)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fYgXiq30R6ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='darkblue'>Operations on word embeddings</font>\n",
        "\n",
        "### Task 1 - Cosine similarity\n",
        "\n",
        "Similarity between two words represented as words $wordA$ and $wordB$ can be measured by their cosine similarity:\n",
        "\n",
        "$$\\text{CosineSimilarity(wordA, wordB)} = \\frac {wordA \\cdot wordB} {||wordA||_2 ||wordB||_2} \\tag{1}$$\n",
        "\n",
        "The above equation returns a value that denotes the level of similarity between $wordA$ and $wordB$. \n",
        "\n",
        "The cosine similarity of $wordA$ and $wordB$ will be close to 1 if the two words are similar, otherwise, the cosine similarity will be small. \n",
        "\n",
        "**Note**: We will be using \"vector or embedding of a word\" and \"word\" interchangeably in this lab. \n",
        "\n",
        "Run the code cell below to execute the formula above. \n",
        "\n"
      ],
      "metadata": {
        "id": "VFF8yxWxq06z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def cosine_similarity(vector1, vector2):\n",
        "    \"\"\"\n",
        "    Calculates the cosine similarity of two word vectors - vector1 and vector2\n",
        "    Arguments:\n",
        "        vector1 (ndarray): A word vector having shape (n,)\n",
        "        vector2 (ndarray): A word vector having shape (n,)\n",
        "    Returns:\n",
        "        cosine_similarity (float): The cosine similarity between vector1 and vector2\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute the dot product between vector1 and vector2 \n",
        "    dot = np.dot(vector1, vector2)                                             \n",
        "\n",
        "    # Compute the Euclidean norm or length of vector1 \n",
        "    norm_vector1 = np.sqrt(np.sum(vector1 ** 2))  \n",
        "\n",
        "    # Compute the Euclidean norm or length of vector2 \n",
        "    norm_vector2 = np.sqrt(np.sum(vector2 ** 2)) \n",
        "\n",
        "    # Compute the cosine similarity as defined in equation 1 \n",
        "    cosine_similarity = dot / (norm_vector1 * norm_vector2)                    \n",
        "\n",
        "    return cosine_similarity\n"
      ],
      "metadata": {
        "id": "QJWkmQXh-iK1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "**<font color='red'>Task 1:</font>** \n",
        "\n",
        "1) Which of the outputs are very similar or dissimilar? Give a reason for each of your answer. \n",
        "\n",
        "2) What can you observe about the last output? \n",
        "***"
      ],
      "metadata": {
        "id": "p2Caebg8cuZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Run this cell to obtain and report your answers\n",
        "man = word_to_vector_map[\"man\"]\n",
        "woman = word_to_vector_map[\"woman\"]\n",
        "cat = word_to_vector_map[\"cat\"]\n",
        "dog = word_to_vector_map[\"dog\"]\n",
        "orange = word_to_vector_map[\"orange\"]\n",
        "england = word_to_vector_map[\"england\"]\n",
        "london = word_to_vector_map[\"london\"]\n",
        "edinburgh = word_to_vector_map[\"edinburgh\"]\n",
        "scotland = word_to_vector_map[\"scotland\"]\n",
        "\n",
        "print(f\"Cosine similarity between man and woman: {cosine_similarity(man, woman)}\")\n",
        "print(f\"Cosine similarity between cat and dog: {cosine_similarity(cat, dog)}\")\n",
        "print(f\"Cosine similarity between cat and cow: {cosine_similarity(cat, orange)}\")\n",
        "print(f\"Cosine similarity between england - london and edinburgh - scotland: {cosine_similarity(england - london, edinburgh - scotland)}\")"
      ],
      "metadata": {
        "id": "eP-Kxp7YBcDw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2 - Word analogy\n",
        "\n",
        "In an analogy task, you are given an analogy in the form \"i is to j as k is to ___\". Your task is to complete this sentence. \n",
        "\n",
        "For example, if you are given \"man is to king as woman is to $l$\" (denoted as $man:king :: woman:l$). You are to find the best word $l$ that answers the analogy the best. Simple arithmetic of the word embeddings will find that $l = queen$ is the best answer because the embedding vectors of words $i$, $j$, $k$, and $l$ denoted as $e_i$, $e_j$, $e_k$, $e_l$ have the following relationship:\n",
        "$$e_j - e_i ≈ e_l - e_k$$\n",
        "\n",
        "Cosine similarity can be used to measure the similarity between $e_j - e_i$ and $e_l - e_k$\n",
        "\n",
        "Run the cell below to setup the analogy task."
      ],
      "metadata": {
        "id": "LmsetI1rFUcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def answer_analogy(word_i, word_j, word_k, word_to_vector_map):\n",
        "    \"\"\"\n",
        "    Performs word analogy as described above\n",
        "    Arguments:\n",
        "        word_i (String): A word\n",
        "        word_j (String): A word\n",
        "        word_k (String): A word\n",
        "        word_to_vector_map (Dict): A dictionary of words as key and its associated embedding vector as value\n",
        "    Returns:\n",
        "        best_word (String): A word that fufils the relationship that e_j - e_i as close as possible to e_l - e_k, as measured by cosine similarity\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert words to lowercase\n",
        "    word_i = word_i.lower()\n",
        "    word_j = word_j.lower()\n",
        "    word_k = word_k.lower()\n",
        "\n",
        "    # Start code here #\n",
        "    try:\n",
        "        # Get the embedding vectors of word_i (~ 1 line)\n",
        "        embedding_vector_of_word_i = word_to_vector_map[word_i]          # Replace with None\n",
        "    except KeyError:\n",
        "        print(f\"{word_i} is not in our vocabulary. Please try a different word.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Get the embedding vectors of word_j (~ 1 line)\n",
        "        embedding_vector_of_word_j = word_to_vector_map[word_j]          # Replace with None\n",
        "    except KeyError:\n",
        "        print(f\"{word_j} is not in our vocabulary. Please try a different word.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Get the embedding vectors of word_k (~ 1 line)\n",
        "        embedding_vector_of_word_k = word_to_vector_map[word_k]          # Replace with None\n",
        "    except KeyError:\n",
        "        print(f\"{word_k} is not in our vocabulary. Please try a different word.\")\n",
        "        return\n",
        "    # End code here #\n",
        "\n",
        "    # Get all the words in our word to vector map i.e our vocabulary\n",
        "    words = word_to_vector_map.keys()\n",
        "    max_cosine_similarity = -1000                           # Initialize to a large negative number\n",
        "    best_word = None                                        # Keep track of the word that best answers the analogy. Note: Do not change this None\n",
        "\n",
        "    # Since we are looping through the whole vocabulary, if we encounter a word \n",
        "    # that is the same as our input, that word becomes the best_word. To avoid \n",
        "    # that we skip the input word. \n",
        "    input_words = set([word_i, word_j, word_k])\n",
        "\n",
        "    for word in words:\n",
        "        if word in input_words:\n",
        "            continue\n",
        "        \n",
        "        # Start code here #\n",
        "        # Compute cosine similarity \n",
        "        similarity = cosine_similarity(\n",
        "            embedding_vector_of_word_j - embedding_vector_of_word_i,\n",
        "            word_to_vector_map[word] - embedding_vector_of_word_k\n",
        "        )                                                # Replace arguments with None\n",
        "\n",
        "        # Have we seen a cosine similarity bigger than max_cosine_similarity?\n",
        "            # then update the max_cosine_similarity to the current cosine similarity\n",
        "            # and update the best_word to the current word (~ 3 lines) \n",
        "        if similarity > max_cosine_similarity:           # Replace with None > None\n",
        "            max_cosine_similarity = similarity           # Replace with max_cosine_similarity = None\n",
        "            best_word = word                             # Replace with best_word = None\n",
        "        # End code here\n",
        "\n",
        "    return best_word"
      ],
      "metadata": {
        "id": "C2k8WuJYn6y9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "**<font color='red'>Task 2:</font>** After running the cell below;\n",
        "\n",
        "\n",
        "1) What do you observe? Were there any cultural or gender stereotypes? List them. \n",
        "\n",
        "2) Does the algorithm always give the right answer? List the incorrect analogies and what the correct analogy is if any. \n",
        "***\n"
      ],
      "metadata": {
        "id": "PzFBmLKKyDD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "analogies = [('france', 'french', 'germany'), \n",
        "             ('england', 'london', 'japan'), \n",
        "             ('boy', 'girl', 'man'), \n",
        "             ('man', 'doctor', 'woman'),\n",
        "             ('small', 'smaller', 'big')]\n",
        "for analogy in analogies:\n",
        "    best_word = answer_analogy(*analogy, word_to_vector_map)\n",
        "    if best_word:\n",
        "        print(f\"{analogy[0]} -> {analogy[1]} :: {analogy[2]} -> {best_word}\")"
      ],
      "metadata": {
        "id": "wSB0BR5YvPg0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3 - Geometry of Gender and Bias in Word Embeddings: Occupational stereotypes \n",
        "In this task, we will understand the biases present in word-embedding i.e which words are closer to $she$ than to $he$. This will be achieved by evaluating whether the pre-trained word embeddings have sterotypes on occupation words. \n",
        "\n",
        "We can determine gender bias by computing the cosine similarity between each occupation word embedding and the embedding vector obtained by subtracting the embedding vector of $he$ from that of $she$ i.e ($she - he$).\n",
        "\n",
        "$$occupation\\_word_i \\cdot ||she - he||_2 \\tag{2}$$\n",
        "\n",
        "Run the cells below to download and view the occupations."
      ],
      "metadata": {
        "id": "3sbFypomLRbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def download_occupations():\n",
        "    if not Path('debiaswe').is_dir():\n",
        "        print(\"Downloading occupation list ...\")\n",
        "        !git clone -q https://github.com/tolga-b/debiaswe.git\n",
        "        print(\"Occupation list downloaded.\")\n",
        "\n",
        "    return '/content/debiaswe/data/professions.json'\n",
        "\n",
        "\n",
        "def view_occupations(occupations_file):\n",
        "    with open(occupations_file, 'r') as file_handle:\n",
        "        occupations = json.load(file_handle)\n",
        "\n",
        "        for occupation in occupations:\n",
        "            print(occupation[0])"
      ],
      "metadata": {
        "id": "Rg5hB08IX8PB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download occupations list"
      ],
      "metadata": {
        "id": "SaFevv0MzaRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "occupations_file = download_occupations()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XXJphb1azP7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "View list of occupations"
      ],
      "metadata": {
        "id": "Qob_1bcOzhp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# View occupations\n",
        "view_occupations(occupations_file)"
      ],
      "metadata": {
        "id": "O1dlznxmliBJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the cell below to setup similarity computation between the word embedding vector of occupation words and the embedding vector difference between $she$ and $he$."
      ],
      "metadata": {
        "id": "Ci7j7szxzF53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def get_occupation_stereotypes(she, he, occupations_file, word_to_vector_map, verbose=False):\n",
        "    \"\"\"\n",
        "    Computes the words that are closest to she and he in the GloVe embeddings\n",
        "    Arguments:\n",
        "        she (String): A word\n",
        "        he (String): A word\n",
        "        occupations_file (String): The path to the occupation file\n",
        "        word_to_vector_map (Dict): A dictionary mapping words to embedding vectors\n",
        "    Returns:\n",
        "        most_similar_words (Tuple(List[Tuple(Float, String)], List[Tuple(Float, String)])): \n",
        "        A tuple of the list of the most similar occupation words to she and he with their associated similarity\n",
        "    \"\"\"\n",
        "\n",
        "    # Read occupations\n",
        "    with open(occupations_file, 'r') as file_handle:\n",
        "        occupations = json.load(file_handle)\n",
        "\n",
        "    # Extract occupation words\n",
        "    occupation_words = [occupation[0] for occupation in occupations]\n",
        "\n",
        "    # Get embedding vector of she \n",
        "    embedding_vector_she = word_to_vector_map[she]                                                        \n",
        "    # Get embedding vector of he                                                               \n",
        "    embedding_vector_he = word_to_vector_map[he]                                                         \n",
        "    # Get the vector difference between embedding vectors of she and he          \n",
        "    vector_difference_she_he = embedding_vector_she - embedding_vector_he                                \n",
        "    # Get the normalized difference \n",
        "    normalized_difference_she_he = vector_difference_she_he / np.linalg.norm(vector_difference_she_he)   \n",
        "\n",
        "    # Store the cosine similarities\n",
        "    similarities = []\n",
        "\n",
        "    for word in occupation_words:\n",
        "        try:\n",
        "            # Get the embedding vector of the current occupation word\n",
        "            occupation_word_embedding_vector = word_to_vector_map[word]                                                                  \n",
        "            # Compute cosine similarity between embedding vector of the occupation word and normalized she - he vector \n",
        "            similarity = np.dot(occupation_word_embedding_vector, normalized_difference_she_he)                                           \n",
        "            similarities.append((similarity, word))\n",
        "        except KeyError:\n",
        "            if verbose:\n",
        "                print(f\"{word} is not in our vocabulary.\")\n",
        "\n",
        "    most_similar_words = sorted(similarities)\n",
        "\n",
        "    return most_similar_words[:20], most_similar_words[-20:]\n",
        "\n"
      ],
      "metadata": {
        "id": "Gngs5ZXINNiS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "**<font color='red'>Task 3:</font>** Execute the cell below and report your results. \n",
        "\n",
        "1) Does the GloVe word embeddings propagate bias? why?\n",
        "\n",
        "2) From the list associated with she, list those that reflect gender stereotype.   \n",
        "\n",
        "3) Compare your list from 2 to the occupations closest to he. What are your conclusions?\n",
        "\n",
        "Exclude businesswoman from your list.\n",
        "***"
      ],
      "metadata": {
        "id": "SbrKEw6NzMea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "he, she = get_occupation_stereotypes('she', 'he', occupations_file, word_to_vector_map)\n",
        "print(\"Occupations closest to he:\")\n",
        "for occupation in he:\n",
        "    print(f\"{occupation[0], occupation[1]}\")\n",
        "\n",
        "print(\"\\nOccupations closest to she:\")\n",
        "for occupation in she:\n",
        "    print(f\"{occupation[0], occupation[1]}\")"
      ],
      "metadata": {
        "id": "66nKdC4Tjwoe",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!--\n",
        "### Task 4 - Analogies exhibiting stereotypes \n",
        "Using analogies to quantify gender stereotype in the embedding. Given two words, e.g. $he$, $she$, generate a pair of words, $x$ and $y$, such that $he$ to $x$ as $she$ to $y$ is a good analogy. This will generate pairs that the embedding believes to be analogous to $he$, $she$ or any other pair of seed words. \n",
        "\n",
        "The analogy generator takes as input a pair of seed words (a, b) which determines the seed direction $\\vec{a} - \\vec{b}$ corresponding to the ***normalized*** difference between the two seed words. \n",
        "\n",
        "In this task, we will use $(a, b) = (she, he)$. Then all pairs of words $x, y$ is scored using the following metric:\n",
        "\n",
        "$$S_{(a, b)}(x, y) = ||\\vec{a} - \\vec{b}||_2 \\cdot ||\\vec{x} - \\vec{y}||_2 \\space \\text{if} \\space ||\\vec{x} - \\vec{y}||_2 ≤ δ, 0 \\space \\text{else}$$\n",
        "\n",
        "Where $δ$ is a threshold for semantic similarity. We will use $δ = 1$. \n",
        "\n",
        "<!-- In other words, the above equation reads, if the normalized difference between $x$ and $y$ is less than or equal to our threshold, then the score of the pair of words $x, y$ is the dot product of the normalized difference between the seed pairs and the normalized difference between the pair of words $x, y$. ->\n",
        "\n",
        "Notice that each vector difference is normalized, therefore we are basically computing the numerator of equation 1 as part of this equation.\n",
        "\n",
        "***\n",
        "**<font color='red'>Task 4:</font>** Test the implementation of your `get_analogies_exhibiting_stereotypes()` below. Report your results. Are the generated analogies biased?\n",
        "***\n",
        "-->\n",
        "\n",
        "### Task 4 - Debiasing word embeddings \n",
        "\n",
        "**Gender Specific words**\n",
        "\n",
        "Words that are associated with a gender by definition. For example, brother, sister, businesswoman or businessman. \n",
        "\n",
        "**Gender neutral words**\n",
        "\n",
        "The remanining words that are not specific to a gender are gender neutral. For example, flight attendant or shoes. The compliment of gender specific words, can be taken as the gender neutral words. \n",
        "\n",
        "**Step 1 - Identify gender subspace i.e the embedding vector (list of numbers) that captures the bias**\n",
        "\n",
        "To robustly estimate bias, we use the gender specific words to learn a gender subpace. To identify the gender subspace, we consider the vector difference of gender specific word pairs, such as $\\vec{she} - \\vec{he}$, $\\vec{woman} - \\vec{man}$ or $\\vec{her} - \\vec{his}$. This identifies a **gender embedding that captures gender**. Aggregating over the word vector differences gives a more accurate embedding vector that captures gender bias. \n",
        "\n",
        "Run the cell below to setup the computation that returns the embedding that captures gender bias.\n"
      ],
      "metadata": {
        "id": "w4dyeTcJxqdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def get_gender_subspace(pairs, word_to_vector_map, num_components=10):\n",
        "    \"\"\"\n",
        "    Compute the gender subspace by computing the principal components of \n",
        "    ten gender pair vectors. \n",
        "    Arguments:\n",
        "        pairs (List[Tuple(String, String)]): A list of gender specific word pairs\n",
        "        word_to_vector_map (Dict): A dictionary mapping words to embedding vectors\n",
        "        num_components (Int): The number of principal components to compute. Defaults to 10\n",
        "    Returns:\n",
        "        gender_subspace (ndarray): The gender bias subspace(or direction) of shape (embedding dimension,)\n",
        "    \"\"\"\n",
        "\n",
        "    matrix = []\n",
        "    for word_1, word_2 in pairs:\n",
        "        embedding_vector_word_1 = word_to_vector_map[word_1]\n",
        "        embedding_vector_word_2 = word_to_vector_map[word_2]\n",
        "        center = (embedding_vector_word_1 + embedding_vector_word_2) / 2\n",
        "        matrix.append(embedding_vector_word_1 - center)\n",
        "        matrix.append(embedding_vector_word_2 - center)\n",
        "    \n",
        "    matrix = np.array(matrix)\n",
        "    pca = PCA(n_components=num_components)\n",
        "    pca.fit(matrix)\n",
        "\n",
        "    pcs = pca.components_                  # Sorted by decreasing explained variance\n",
        "    eigenvalues = pca.explained_variance_  # Eigenvalues \n",
        "    gender_subspace = pcs[0]               # The first element has the highest eigenvalue\n",
        "    return gender_subspace"
      ],
      "metadata": {
        "id": "C2gVI7TvcvDp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to view the gender embedding "
      ],
      "metadata": {
        "id": "SyPpcaMFAiAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "gender_specific_pairs = [\n",
        "    ('she', 'he'),\n",
        "    ('her', 'his'),\n",
        "    ('woman', 'man'),\n",
        "    ('mary', 'john'),\n",
        "    ('herself', 'himself'),\n",
        "    ('daughter', 'son'),\n",
        "    ('mother', 'father'),\n",
        "    ('gal', 'guy'),\n",
        "    ('girl', 'boy'),\n",
        "    ('female', 'male')\n",
        "]\n",
        "gender_direction = get_gender_subspace(gender_specific_pairs, word_to_vector_map)\n",
        "print(f'Gender embedding is: \\n{gender_direction}')"
      ],
      "metadata": {
        "id": "3qsDDbxTgYSO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " \n",
        "***\n",
        "**<font color='red'>Task 4a:</font>** Running the cell below computes the similarity between the gender embedding and the embedding vectors of male and female names. What can you observe about the names? \n",
        "***"
      ],
      "metadata": {
        "id": "2dqlFhLsXhsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "print('Names and their similarities')\n",
        "names = [\"mary\", \"john\", \"sweta\", \"david\", \"kazim\", \"angela\"]\n",
        "for name in names:\n",
        "    print(name, cosine_similarity(gender_direction, word_to_vector_map[name]))"
      ],
      "metadata": {
        "id": "V0dQ7vU6XRqs",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "**<font color='red'>Task 4b:</font>** Quantify direct and indirect biases between words and the gender embedding by running the following cell. What is your observation?\n",
        "***"
      ],
      "metadata": {
        "id": "7-3f7NiKZHRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "words = [\"engineer\", \"science\", \"pilot\", \"technology\", \"lipstick\", \"arts\", \"singer\", \"computer\", \"receptionist\", \"fashion\", \"doctor\", \"literature\"]\n",
        "for word in words:\n",
        "    print(word, cosine_similarity(gender_direction, word_to_vector_map[word]))"
      ],
      "metadata": {
        "id": "v9b4NpyZZR-K",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 - Neutralize gender neutral words**\n",
        "\n",
        "Words such as \"babysit\" should remain neutral in association with gender specific words. \"babysit\" shouldn't be more closer to \"grandmother\" than to \"grandfather\". Take college admission as an example, we do not want the embedding of a candidate to be prefered over another by gender. In such cases it would be a good idea to neutralize the candidates or \"babysit\" by ensuring the numbers in their embedding list is zero in the gender subspace. \n",
        "\n",
        "Run the cell below to setup neutralization. "
      ],
      "metadata": {
        "id": "H81h64YIae53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def neutralize(word, gender_direction, word_to_vector_map):\n",
        "    \"\"\"\n",
        "    Project the vector of word onto the gender subspace to remove the bias of \"word\"\n",
        "    Arguments:\n",
        "        word (String): A word to debias\n",
        "        gender_direction (ndarray): Numpy array of shape (embedding size (50), ) which is the bias axis\n",
        "        word_to_vector_map (Dict): A dictionary mapping words to embedding vectors\n",
        "\n",
        "    Returns:\n",
        "        debiased_word (ndarray): the vector representation of the neutralized input word \n",
        "    \"\"\"\n",
        "\n",
        "    # Get the vector representation of word \n",
        "    embedding_of_word = word_to_vector_map[word]\n",
        "\n",
        "    # Compute the projection of word onto gender direction \n",
        "    projection_of_word_onto_gender = (np.dot(embedding_of_word, gender_direction) * gender_direction) / np.sum(gender_direction ** 2)\n",
        "\n",
        "    # Neutralize word\n",
        "    debiased_word  = embedding_of_word - projection_of_word_onto_gender\n",
        "\n",
        "    return debiased_word"
      ],
      "metadata": {
        "id": "6QQWRKbX2k5R",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "***\n",
        "**<font color='red'>Task 4c:</font>** Run the cell below to neutralize the gender neutral word \"babysit\". What is your observation?\n",
        "***"
      ],
      "metadata": {
        "id": "8UqUM48Q9mGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "word = \"babysit\"\n",
        "print(f\"Before neutralization, cosine similarity between {word} and embedding that captures gender bias is: \\n{cosine_similarity(word_to_vector_map[word], gender_direction)}\")\n",
        "\n",
        "debiased_word = neutralize(word, gender_direction, word_to_vector_map)\n",
        "print(f\"After neutralization, cosine similarity between {word} and embedding that captures gender bias  is: \\n{cosine_similarity(debiased_word, gender_direction)}\")"
      ],
      "metadata": {
        "id": "IBlXaobP8k3s",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3 - Equalize**\n",
        "\n",
        "Neutralization debiases gender neutral words, however, it does not respect the definition of gender specific words. Equalization fixes this by ensuring that all neutral words are equidistant to all words in pair or set of gender specific words. \n",
        "\n",
        "Going back to our \"babysit\" example, if {grandmother, grandfather} and {guy, gal} are two gender specific sets, then after performing equalization, \"babysit\" should be of equal distance to $grandmother$ and $granfather$ and equal distance to $guy$ and $gal$. But closer to the grandparents and further from the $guy$ and $gal$. \n",
        "\n",
        "This handles the situation where bias doesn't be displayed with respect to neutral words. \n",
        "\n",
        "Run the cell below to setup neutralization computation. "
      ],
      "metadata": {
        "id": "Tph-OAkfh9O2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def equalization(equality_set, bias_direction, word_to_vector_map):\n",
        "    \"\"\"\n",
        "    Equalize the pair of gender specific words in the equality set ensuring that\n",
        "    any neutral word is equidistant to all words in the equality set. \n",
        "    Arguments:\n",
        "        equality_set (Tuple(String, String)): a tuple of strings of gender specific\n",
        "        words to debias e.g (\"grandmother\", \"grandfather\")\n",
        "        bias_direction (ndarray): numpy array of shape (embedding dimension,). The \n",
        "        embedding vector representing the bias direction\n",
        "        word_to_vector_map (Dict):  A dictionary mapping words to embedding vectors\n",
        "    Returns:\n",
        "        embedding_word_a (ndarray): numpy array of shape (embedding dimension,). The \n",
        "        embedding vector representing the first word\n",
        "        embedding_word_b (ndarray): numpy array of shape (embedding dimension,). The \n",
        "        embedding vector representing the second word\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the vector representation of word pair\n",
        "    word_a, word_b = equality_set                       \n",
        "    embedding_word_a = word_to_vector_map[word_a]       \n",
        "    embedding_word_b = word_to_vector_map[word_b]       \n",
        "\n",
        "    # Compute the mean (eq. 5) of embedding_word_a and embedding_word_a \n",
        "    mean = (embedding_word_a + embedding_word_b) / 2    \n",
        "\n",
        "    # Compute the projection of mean representation onto the bias direction (eq. 6) \n",
        "    mean_B = (np.dot(mean, bias_direction) / np.sum(bias_direction ** 2)) * bias_direction\n",
        "\n",
        "    # Compute the projection onto the orthogonal subspace (eq. 7) \n",
        "    mean_othorgonal = mean - mean_B\n",
        "\n",
        "    # Compute the projection of th embedding of word a onto the bias direction (eq. 8) \n",
        "    embedding_word_a_on_bias_direction = (np.dot(embedding_word_a, bias_direction) / np.sum(bias_direction ** 2)) * bias_direction\n",
        "\n",
        "    # Compute the projection of th embedding of word b onto the bias direction (eq. 9) \n",
        "    embedding_word_b_on_bias_direction = (np.dot(embedding_word_b, bias_direction) / np.sum(bias_direction ** 2)) * bias_direction\n",
        "\n",
        "    # Re-embed embedding of word a using eq. 10 \n",
        "    new_embedding_word_a_on_bias_direction = (np.sqrt(np.abs(1 - np.sum(mean_othorgonal ** 2)))) * ((embedding_word_a_on_bias_direction - mean_B) / np.linalg.norm((embedding_word_a - mean_othorgonal) - mean_B))\n",
        "\n",
        "    # Re-embed embedding of word b using eq. 11 \n",
        "    new_embedding_word_b_on_bias_direction = (np.sqrt(np.abs(1 - np.sum(mean_othorgonal ** 2)))) * ((embedding_word_b_on_bias_direction - mean_B) / np.linalg.norm((embedding_word_b - mean_othorgonal) - mean_B))\n",
        "\n",
        "    # Equalize embedding of word a using eq. 12 \n",
        "    embedding_word_a =  mean_othorgonal + new_embedding_word_a_on_bias_direction\n",
        "\n",
        "    # Equalize embedding of word b using eq. 13 \n",
        "    embedding_word_b = mean_othorgonal + new_embedding_word_b_on_bias_direction\n",
        "\n",
        "    return embedding_word_a, embedding_word_b"
      ],
      "metadata": {
        "id": "vMO9XKDf0nO5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below perform equalization of the pair (father, mother)."
      ],
      "metadata": {
        "id": "9lrYzSdlA_Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "print(\"Cosine similarity before equalization:\")\n",
        "print(f\"(embedding vector of father, gender_direction): {cosine_similarity(word_to_vector_map['father'], gender_direction)}\")\n",
        "print(f\"(embedding vector of mother, gender_direction): {cosine_similarity(word_to_vector_map['mother'], gender_direction)}\")\n",
        "print()\n",
        "\n",
        "embedding_word_a, embedding_word_b  = equalization((\"father\", \"mother\"), gender_direction, word_to_vector_map)\n",
        "print(\"Cosine similarity after equalization:\")\n",
        "print(f\"(embedding vector of father, gender_direction): {cosine_similarity(embedding_word_a, gender_direction)}\")\n",
        "print(f\"(embedding vector of mother, gender_direction): {cosine_similarity(embedding_word_b, gender_direction)}\")"
      ],
      "metadata": {
        "id": "dC88RE6MBIpV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "**<font color='red'>Task 5a:</font>** Looking at the equalization, what can you observe?.\n",
        "***"
      ],
      "metadata": {
        "id": "KJUPPz80BJCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References**:\n",
        " - The debiasing algorithm is from Bolukbasi et al., 2016 [Man is to Computer Programmer as Woman is to Homemake? Debiasing word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)\n",
        " - The code is partly adapted from Andrew Ng's debiasing word embeddings course on [Coursera](https://www.coursera.org/learn/nlp-sequence-models/lecture/zHASj/debiasing-word-embeddings)\n",
        " - The GloVe word embeddings is publicly available at (https://nlp.stanford.edu/projects/glove/) and is due to the works of Jeffrey Pennington, Richard Socher, and Christopher D. Manning. "
      ],
      "metadata": {
        "id": "07sW3hQeZY36"
      }
    }
  ]
}