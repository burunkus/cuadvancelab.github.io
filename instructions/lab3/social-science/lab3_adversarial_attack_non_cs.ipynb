{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvDtBboLkHL7"
      },
      "source": [
        "# Adversarial Attack on Cyberharassment Models\n",
        "\n",
        "In lab 3 (Adversarial attack on cyberharassment models), you will add noise to the input of a cyberharassment model to make the model misclassify the input. This lab will walk you through this and will develop your intuition about adversarial attacks in AI models. \n",
        "\n",
        "**You will learn:**\n",
        "- How to use an adversarial attack algorithm to fool a model trained to detect images that are not safe for work (NSFW).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ehoe18ckedt"
      },
      "source": [
        "## Required modules \n",
        "First, run the cells below to import important packages needed for this lab. The second cell mounts Google drive which is where some files required for the lab will be stored. You will be prompted for authentication, please follow the instruction to authenticate. \n",
        "- [numpy](https://www.numpy.org/) a Python package for scientific computing.\n",
        "- [matplotlib](http://matplotlib.org) a Python library for visualization\n",
        "- [PIL](http://www.pythonware.com/products/pil/) a Python package for image processing\n",
        "- [torch](https://pytorch.org/) a Python open source source framework for deep learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qRXFJevgjeTa"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "seed_val = 42\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3yGMuB32MBa"
      },
      "source": [
        "## Lab Overview\n",
        "\n",
        "<img src=\"https://clemson.box.com/shared/static/lp073durs1970cvy9cboy5lwfytw6awh.png\">\n",
        "\n",
        "You are given a model that have been trained (pretrained) to detect not safe for work (NSFW) images. You are\n",
        "also given a set pf NSFW test images. Of course building such models are important but as you will see in the\n",
        "lab, such a model can be fooled to not detect such contents.\n",
        "\n",
        "You will use adversarial algorithms to generate noise that will be added to the test images. When this modified\n",
        "test image is passed through the model, the model will classify it as not NSFW even though it is.\n",
        "\n",
        "**About test data**\n",
        "\n",
        "We will use some NSFW images obtained by downloading images using the [ nsfw_data_scrapper](https://github.com/EBazarov/nsfw_data_source_urls). Each image belong to 1 of 5 categories (neutral, hentai, drawings, porn and sexy). Each of the categories will be a folder containing images that belong to that category.\n",
        "\n",
        "Let us explore the test dataset. Download the data by running the cell below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3DHnXUQ4URB1"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "%%capture\n",
        "# Download the dataset\n",
        "if not Path('test').is_dir():\n",
        "    !wget https://buffalo.box.com/shared/static/x1wqhcvq46ux4uwlzd3df3uy1qh1n87h.zip\n",
        "    # Unzip it\n",
        "    !unzip -q x1wqhcvq46ux4uwlzd3df3uy1qh1n87h.zip -d test/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRNT9hJQG_zh"
      },
      "source": [
        "It is important to keep the dimension of all images the same, this allows to group a batch of images with the same shape into a single matrix/vector. We will perform some transformations on each image, for each image we make it a specific height and weight (224 x 224), center it, convert it to a tensor (converting the pixels into a vector) and normalize it. In this lab, we will batch our images into groups of 1 i.e each image is in its own batch. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1i2keCjrHM6U"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "image_path = \"/content/test/nsfw_test\"\n",
        "batch_size = 1\n",
        "test_image_transforms = transforms.Compose([transforms.Resize(224),\n",
        "                                            transforms.CenterCrop(224),\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize(\n",
        "                                                mean=[0.485, 0.456, 0.406],\n",
        "                                                std=[0.229, 0.224, 0.225])    \n",
        "                                        ])\n",
        "\n",
        "dataset = datasets.ImageFolder(image_path, transform=test_image_transforms)\n",
        "class_dict = dataset.class_to_idx\n",
        "index_to_class = {}\n",
        "for category, idx in class_dict.items():\n",
        "    index_to_class[idx] = category\n",
        "\n",
        "# Batch dataset and create an iterator over the dataset\n",
        "test_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_WEDCpfk8j3M"
      },
      "outputs": [],
      "source": [
        "#@title View few images. { run: \"auto\" }\n",
        "#@markdown Select a number to view the image and its label. Note: Run manually first by clicking the play button to the left. After this manual run,  each subsequent value selection will run automatically.\n",
        "\n",
        "index = \"0\" #@param [0, 1, 2, 3, 4, 5, 41, 42, 43, 44, 45]\n",
        "image, label = dataset[int(index)]\n",
        "plt.imshow(image.transpose(2, 0))\n",
        "print(f\"Classes = {[category for category in class_dict.keys()]}\")\n",
        "print(f'class = {label}, which is {index_to_class[label]}:')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDaIyX5_oVOp"
      },
      "source": [
        "## NSFW Detection Without Attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mvkdpnvIojYW"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def detect_nsfw(model, test_dataloader):\n",
        "    \"\"\"\n",
        "    Performs the FGSM/PGD attack on a dataset given a specific epsilon value\n",
        "    Args:\n",
        "        model (nn.Module): The neural network under attack\n",
        "        test_dataloader (DataLoader): an iterable over the dataset\n",
        "    Returns:\n",
        "        final_acc, examples (Tuple): a tuple of the final accuracy \n",
        "        of the attack and some adversarial examples for visualization\n",
        "    \"\"\"\n",
        "    \n",
        "    # We are not training, so set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Accuracy counter\n",
        "    correct = 0\n",
        "    examples = []\n",
        "\n",
        "    # Loop over all examples in test set\n",
        "    for data, target in test_dataloader:                         \n",
        "\n",
        "        # Send the data and label to the device\n",
        "        data, target = data.to(device), target.to(device)         \n",
        "\n",
        "        # Forward pass the data through the model\n",
        "        output = model(data)\n",
        "\n",
        "        prediction = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "\n",
        "        # Check for success\n",
        "        if prediction.item() == target.item():\n",
        "            correct += 1\n",
        "            # Special case for saving 0 epsilon examples\n",
        "            if (len(examples) < 5):\n",
        "                example = data.squeeze().detach().cpu().numpy()\n",
        "                if (prediction.item() == 0 or  prediction.item() == 2):\n",
        "                    examples.append( (prediction.item(), target.item(), example) )\n",
        "        else:\n",
        "            # Save some examples for visualization later\n",
        "            if len(examples) < 5:\n",
        "                example = data.squeeze().detach().cpu().numpy()\n",
        "                if (prediction.item() == 0 or  prediction.item() == 2):\n",
        "                    examples.append( (prediction.item(), target.item(), example) )\n",
        "\n",
        "    # Calculate final accuracy for this epsilon\n",
        "    final_acc = correct / float(len(test_dataloader))\n",
        "    print(\"Accuracy = {} / {} = {}\".format(correct, len(test_dataloader), final_acc))\n",
        "\n",
        "    # Return the accuracy and an adversarial example\n",
        "    return final_acc, examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nyMgOD3j_iX"
      },
      "source": [
        "## White-box Attack\n",
        "\n",
        "In a white-box attack, the adversary (the agent generating the adversarial examples) have knowledge about the target model architecture, its parameters and training data. We will be performing a white-box attack since we have a model that have been trained to detect NSFW, we have access to its parameters and we know the data it was trained on. \n",
        "\n",
        "Execute the following cells to setup our model.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rZo4_Fmak04y"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def get_model():\n",
        "    \"\"\"\n",
        "    Initialize the NSFW model which is a modified ResNet50 model with\n",
        "    two additional fully-connected layers\n",
        "    Args:\n",
        "        None\n",
        "    Returns:\n",
        "        model (nn.Module): pre-trained NSFW model\n",
        "    \"\"\"\n",
        "\n",
        "    # NSFW model uses ResNet50 with additional layers \n",
        "    # Load ResNet50 model\n",
        "    model = models.resnet50(pretrained=False)\n",
        "    number_of_fully_connected_features = model.fc.in_features\n",
        "\n",
        "    # Replacing last layer \n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Linear(number_of_fully_connected_features, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(512, 10),\n",
        "        nn.LogSoftmax(dim=1)\n",
        "    )\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7KHbXz2my3g"
      },
      "source": [
        "Download the model parameters (pre-trained weights) by running the cell below. \n",
        "We will make use of a pretrained NSFW [model](https://github.com/emiliantolo/pytorch_nsfw_model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c1d5KCbbbvK3"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "%%capture\n",
        "if not os.path.isfile('ResNet50_nsfw_model.pth'):\n",
        "    !wget https://github.com/emiliantolo/pytorch_nsfw_model/blob/master/ResNet50_nsfw_model.pth?raw=true -O ResNet50_nsfw_model.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8yELK7qnBKT"
      },
      "source": [
        "Initialize the model and load its parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tEsOt5Ktnvq9"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Use GPU if it is available \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "pre_trained_model_path = \"/content/ResNet50_nsfw_model.pth\"\n",
        "# Initialize the network\n",
        "model = get_model().to(device)\n",
        "\n",
        "# Initialize the network with the pretrained weights (parameters that the previous model learned) \n",
        "model.load_state_dict(torch.load(pre_trained_model_path, map_location='cpu')) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2PSTiBiq8mq"
      },
      "source": [
        "## Attack\n",
        "We will use fast gradient sign method (FGSM) to generate adversarial perturbations to fool the target model into predicting the wrong output. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2NNf5cXSr4Y3"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def fgsm(image, epsilon, data_grad):\n",
        "    \"\"\"\n",
        "    Perform the FGSM attack on a single image\n",
        "    Args:\n",
        "        image (torch.tensor): The image to be perturbed\n",
        "        epsilon (float): Hyperparameter for controlling the scale of perturbation\n",
        "        data_grad (): The gradient of the loss wrt to image\n",
        "    Returns:\n",
        "        perturbed_image (torch.tensor): a perturbed image\n",
        "    \"\"\"\n",
        "    # Collect the element-wise sign of the data gradient\n",
        "    sign_data_grad = data_grad.sign()\n",
        "    \n",
        "    # Create the perturbed image by adjusting each pixel of the input image\n",
        "    perturbed_image = image + epsilon * sign_data_grad\n",
        "    \n",
        "    # Adding clipping to maintain [0,1] range\n",
        "    if epsilon != 0.0:\n",
        "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "    \n",
        "    # Return the perturbed image\n",
        "    return perturbed_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR5lpaxrg57e"
      },
      "source": [
        "FGSM Attack\n",
        "- Perform a forward pass through the model using the original image\n",
        "- Perform an FGSM attack by using FGSM to generate an adversarial image\n",
        "- Perform a forward pass through the model using the adversarial image "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C0CcczuvtVg6"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def fgsm_attack(model, test_dataloader, epsilon):\n",
        "    \"\"\"\n",
        "    Performs the FGSM/PGD attack on a dataset given a specific epsilon value\n",
        "    Args:\n",
        "        model (nn.Module): The neural network under attack\n",
        "        device (): put variables into cpu/gpu mode\n",
        "        test_dataloader (DataLoader): an iterable over the dataset\n",
        "        epsilon (float): hyperparameter for controlling the scale of perturbation \n",
        "        attack_type (String): adversarial attack method, defaults to FGSM\n",
        "    Returns:\n",
        "        final_acc, adversarial_examples (Tuple): a tuple of the final accuracy \n",
        "        of the attack and some adversarial examples for visualization\n",
        "    \"\"\"\n",
        "    \n",
        "    # We are not training, so set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Accuracy counter\n",
        "    correct = 0\n",
        "    adversarial_examples = []\n",
        "\n",
        "    # Loop over all examples in test set\n",
        "    for data, target in test_dataloader:                         \n",
        "\n",
        "        # Send the data and label to the device\n",
        "        data, target = data.to(device), target.to(device)         \n",
        "\n",
        "        # Set requires_grad attribute of tensor. Important for Attack\n",
        "        data.requires_grad = True\n",
        "\n",
        "        # Forward pass the data through the model\n",
        "        output = model(data)\n",
        "\n",
        "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "\n",
        "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
        "        if init_pred.item() != target.item():\n",
        "            continue\n",
        "\n",
        "        # Calculate the loss\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Zero all existing gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Calculate gradients of model in backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Collect datagrad\n",
        "        data_grad = data.grad.data\n",
        "\n",
        "        # Call FGSM to add perturbation to the data\n",
        "        perturbed_data = fgsm(data, epsilon, data_grad)\n",
        "\n",
        "        # Re-classify the perturbed image\n",
        "        output = model(perturbed_data)\n",
        "\n",
        "        # Check for success\n",
        "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if final_pred.item() == target.item():\n",
        "            correct += 1\n",
        "            # Special case for saving 0 epsilon examples\n",
        "            if (epsilon == 0.0) and (len(adversarial_examples) < 5):\n",
        "                adversarial_example = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "                if (init_pred.item() == 0 and final_pred.item() == 0) or \\\n",
        "                (init_pred.item() == 2 and final_pred.item() == 2):\n",
        "                    adversarial_examples.append( (init_pred.item(), final_pred.item(), adversarial_example) )\n",
        "        else:\n",
        "            # Save some adversarial examples for visualization later\n",
        "            if len(adversarial_examples) < 5:\n",
        "                adversarial_example = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "                if (init_pred.item() == 0 and final_pred.item() == 0) or \\\n",
        "                (init_pred.item() == 0 and final_pred.item() == 2) or \\\n",
        "                (init_pred.item() == 2 and final_pred.item() == 0) or \\\n",
        "                (init_pred.item() == 2 and final_pred.item() == 1):\n",
        "                    adversarial_examples.append( (init_pred.item(), final_pred.item(), adversarial_example) )\n",
        "\n",
        "    # Calculate final accuracy for this epsilon\n",
        "    final_acc = correct / float(len(test_dataloader))\n",
        "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_dataloader), final_acc))\n",
        "\n",
        "    # Return the accuracy and an adversarial example\n",
        "    return final_acc, adversarial_examples, epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K5YvkuAiBRc"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pjkKMbum37qa"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def plot_accuracy_vs_epsilon(epsilons, accuracies):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.plot(epsilons, accuracies, \"*-\")\n",
        "    plt.yticks(np.arange(0, 1.1, step=0.1))\n",
        "    plt.xticks(np.arange(0, .35, step=0.05))\n",
        "    plt.title(\"Accuracy vs Epsilon\")\n",
        "    plt.xlabel(\"Epsilon\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OjkLnNEE4POK"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Plot several examples of adversarial samples at each epsilon. Note: Only showing images from the drawings and neutral classes to avoid showing inappropriate content\n",
        "def plot_adversarial_samples(epsilons, adversarial_examples):\n",
        "    count = 0\n",
        "    \n",
        "    plt.figure(figsize=(8,10))\n",
        "    for i in range(len(epsilons)):\n",
        "        for j in range(len(adversarial_examples[i])):\n",
        "            count += 1\n",
        "            plt.subplot(len(epsilons),len(adversarial_examples[0]), count)\n",
        "            plt.xticks([], [])\n",
        "            plt.yticks([], [])\n",
        "            if j == 0:\n",
        "                plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n",
        "            orig, adv, ex = adversarial_examples[i][j]\n",
        "            # Convert class indexes to string \n",
        "            orig = index_to_class[orig]\n",
        "            adv = index_to_class[adv]\n",
        "            plt.title(\"{}\\n -> {}\".format(orig, adv))\n",
        "            plt.imshow(np.transpose(ex, (1,2,0)), cmap=\"gray\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ325LF5rmQS"
      },
      "source": [
        "## Execute Without FGSM Attack\n",
        "\n",
        "Perform NSFW detection with the trained model.\n",
        "\n",
        "- - - -\n",
        "**<font color='red'>Task 1:</font>\n",
        "What are your observations in terms of how the model is predicting NSFW. Is the model predicting NSFW accurately?**\n",
        "- - - -\n",
        "\n",
        "Run the cell below to answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fGl-N9mrvX3"
      },
      "outputs": [],
      "source": [
        "accuracy, _ = detect_nsfw(model, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKG4zM8AuFxZ"
      },
      "source": [
        "## Execute FGSM Attack\n",
        "\n",
        "$ϵ$ controls the scale of the perturbations and we will use different $ϵ$ values to understand its effect. \n",
        "\n",
        "Run the cell below to setup for the next task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "b7VqPsEwth5M"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "accuracies = []\n",
        "adversarial_examples = []\n",
        "epsilons = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqcw1HjzuSOz"
      },
      "source": [
        "- - - -\n",
        "**<font color='red'>Task 2:</font>**\n",
        "- First, execute the cell below with the default epsilon of 0.0\n",
        "- Select each of the other epsilon value. The code cell will execute automatically\n",
        "- Execute the visualize and sample adversarial examples code cells\n",
        "- Compare the result with the result of executing without FSGM attack above and describe your observations. What effects does the epsilon value have on accuracy and perceptibility of the examples?\n",
        "- - - -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nZrEFSVjuKW7"
      },
      "outputs": [],
      "source": [
        "#@title Choose epsilon and execute. { run: \"auto\" }\n",
        "#@markdown Select an epsilon value. Note: Run manually first by clicking the play button to the left. After this manual run,  each subsequent value selection will run automatically.\n",
        "epsilon = \"0.0\"  #@param [\"0.0\", \"0.02\", \"0.04\", \"0.06\", \"0.08\", \"0.1\", \"0.14\"]\n",
        "\n",
        "# Perform attack using each epsilon value\n",
        "accuracy, adversarial_example, epsilon = fgsm_attack(model, test_dataloader, float(epsilon))\n",
        "accuracies.append(accuracy)\n",
        "epsilons.append(epsilon)\n",
        "adversarial_examples.append(adversarial_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8evjbnIz345i"
      },
      "source": [
        "Visualize accuracy vs epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NdHJOABpkzi9"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "plot_accuracy_vs_epsilon(epsilons, accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlTp8cuC4Hlo"
      },
      "source": [
        "Sample adversarial examples - Original prediction -> adversarial prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-8RQyILklXBC"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "plot_adversarial_samples(epsilons, adversarial_examples)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
